{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c9b349-2f42-4b8f-b8d5-a1f64c5a5735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_nndct\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_nndct import Inspector\n",
    "batch_size = 1\n",
    "# load data\n",
    "data = pd.read_pickle(\"RML2016.10a_dict.pkl\", compression='infer')\n",
    "qpsk_2_data_all = data[('QPSK', 2)]\n",
    "bpsk_2_data_all = data[('BPSK', 2)]\n",
    "\n",
    "# labels\n",
    "qpsk_labels = [0] * 1000  # QPSK = 0\n",
    "bpsk_labels = [1] * 1000  # BPSK = 1\n",
    "\n",
    "# combine the data lables\n",
    "data_combined = np.concatenate((qpsk_2_data_all, bpsk_2_data_all), axis=0)\n",
    "labels_combined = qpsk_labels + bpsk_labels\n",
    "\n",
    "# convert labels to NumPy array and then to PyTorch tensor with Long data type\n",
    "labels_combined = np.array(labels_combined, dtype=np.int64)\n",
    "labels_combined = torch.from_numpy(labels_combined).long()\n",
    "\n",
    "# convert 2 PyTorch tensor\n",
    "data_combined = torch.from_numpy(data_combined).float()\n",
    "\n",
    "# convert labels 2 NumPy array and then 2 PyTorch tensor\n",
    "labels_combined = np.array(labels_combined)\n",
    "labels_combined = torch.from_numpy(labels_combined)\n",
    "\n",
    "# break into training + testing\n",
    "test_size = 0.2  # Adjust the test size as needed\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data_combined, labels_combined, test_size=test_size, random_state=42)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # access a single data sample and label\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "    \n",
    "        # Convert sample, min_vals, and max_vals to PyTorch tensors\n",
    "        sample = torch.tensor(sample, dtype=torch.float32)\n",
    "        min_vals = torch.tensor(sample.min(axis=1).values, dtype=torch.float32)\n",
    "        max_vals = torch.tensor(sample.max(axis=1).values, dtype=torch.float32)\n",
    "        \n",
    "        #normalize\n",
    "        epsilon = 1e-10\n",
    "        normalized_sample = 2 * (sample - min_vals.unsqueeze(1)) / (max_vals.unsqueeze(1) - min_vals.unsqueeze(1) + epsilon) - 1\n",
    "    \n",
    "        return normalized_sample, label\n",
    "\n",
    "train_dataset = MyDataset(data_train, labels_train)\n",
    "test_dataset = MyDataset(data_test, labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42d3539-5048-4d23-956d-0443b85e086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1D(\n",
      "  (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        in_channels = 2\n",
    "        in_features = 128\n",
    "        \n",
    "        # define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # efine the fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * (in_features // 4), 256)  # in_features // 4 due to max pooling\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, 2, 128)\n",
    "        \n",
    "        # convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # flatten (before fully connected layers)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize model\n",
    "num_classes = 2 #BPSK / QPSK\n",
    "model = CNN1D(num_classes)\n",
    "\n",
    "print(model) #model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c54e2d2-fcb2-4a71-b863-f7aa59a73add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Inspector is on.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Start to inspect model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing CNN1D...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model nndct_st_CNN1D_ed is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/822561179.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(sample, dtype=torch.float32)\n",
      "/tmp/ipykernel_118/822561179.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  min_vals = torch.tensor(sample.min(axis=1).values, dtype=torch.float32)\n",
      "/tmp/ipykernel_118/822561179.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_vals = torch.tensor(sample.max(axis=1).values, dtype=torch.float32)\n",
      "██████████████████████████████████████████████████| 13/13 [00:00<00:00, 2666.83it/s, OpInfo: name = return_0, t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(inspect/CNN1D.py)\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN]: CNN1D::308 is not tensor.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Find subgraph for convlike_fix_18:\n",
      "node name:CNN1D::CNN1D/Linear[fc1]/ret.19, op type:nndct_dense, output shape: [1, 256]\n",
      "node name:CNN1D::CNN1D/ReLU[relu3]/ret.21, op type:nndct_relu, output shape: [1, 256]\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Find subgraph for reshape_fix_1:\n",
      "node name:CNN1D::CNN1D/MaxPool1d[maxpool2]/ret.13_sink_transpose_0, op type:nndct_permute, output shape: [1, 128, 32]\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Find subgraph for reshape_fix_1:\n",
      "node name:CNN1D::CNN1D/ret.17, op type:nndct_reshape, output shape: [1, 4096]\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Find subgraph for convlike_fix_20:\n",
      "node name:CNN1D::CNN1D/Linear[fc2]/ret, op type:nndct_dense, output shape: [1, 2]\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "I20240215 10:18:54.841639   118 compile_pass_manager.cpp:352] [UNILOG][INFO] Compile mode: dpu\n",
      "I20240215 10:18:54.841661   118 compile_pass_manager.cpp:353] [UNILOG][INFO] Debug mode: null\n",
      "I20240215 10:18:54.841665   118 compile_pass_manager.cpp:357] [UNILOG][INFO] Target architecture: DPUCAHX8L_ISA0_SP\n",
      "I20240215 10:18:54.841737   118 compile_pass_manager.cpp:465] [UNILOG][INFO] Graph name: nndct_dense_nndct_relu_gbpUSIH6h2JmT3Li, with op num: 9\n",
      "I20240215 10:18:54.841740   118 compile_pass_manager.cpp:478] [UNILOG][INFO] Begin to compile...\n",
      "I20240215 10:18:54.874176   118 compile_pass_manager.cpp:489] [UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "I20240215 10:18:54.874205   118 compile_pass_manager.cpp:504] [UNILOG][INFO] Compile done.\n",
      "I20240215 10:18:54.878021   118 compile_pass_manager.cpp:352] [UNILOG][INFO] Compile mode: dpu\n",
      "I20240215 10:18:54.878032   118 compile_pass_manager.cpp:353] [UNILOG][INFO] Debug mode: null\n",
      "I20240215 10:18:54.878036   118 compile_pass_manager.cpp:357] [UNILOG][INFO] Target architecture: DPUCAHX8L_ISA0_SP\n",
      "I20240215 10:18:54.878100   118 compile_pass_manager.cpp:465] [UNILOG][INFO] Graph name: nndct_permute_hg5eZdpz1kRFvSfM, with op num: 4\n",
      "I20240215 10:18:54.878104   118 compile_pass_manager.cpp:478] [UNILOG][INFO] Begin to compile...\n",
      "W20240215 10:18:54.879339   118 PartitionPass.cpp:4160] [UNILOG][WARNING] xir::Op{name = CNN1D__CNN1D_MaxPool1d_maxpool2__ret_13_sink_transpose_0, type = transpose} has been assigned to CPU.\n",
      "I20240215 10:18:54.879390   118 compile_pass_manager.cpp:489] [UNILOG][INFO] Total device subgraph number 2, DPU subgraph number 0\n",
      "I20240215 10:18:54.879403   118 compile_pass_manager.cpp:504] [UNILOG][INFO] Compile done.\n",
      "I20240215 10:18:54.881002   118 compile_pass_manager.cpp:352] [UNILOG][INFO] Compile mode: dpu\n",
      "I20240215 10:18:54.881009   118 compile_pass_manager.cpp:353] [UNILOG][INFO] Debug mode: null\n",
      "I20240215 10:18:54.881012   118 compile_pass_manager.cpp:357] [UNILOG][INFO] Target architecture: DPUCAHX8L_ISA0_SP\n",
      "I20240215 10:18:54.881063   118 compile_pass_manager.cpp:465] [UNILOG][INFO] Graph name: nndct_reshape_lyaSW6pCrXYn79bg, with op num: 7\n",
      "I20240215 10:18:54.881065   118 compile_pass_manager.cpp:478] [UNILOG][INFO] Begin to compile...\n",
      "I20240215 10:18:54.882206   118 compile_pass_manager.cpp:489] [UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "I20240215 10:18:54.882222   118 compile_pass_manager.cpp:504] [UNILOG][INFO] Compile done.\n",
      "I20240215 10:18:54.886629   118 compile_pass_manager.cpp:352] [UNILOG][INFO] Compile mode: dpu\n",
      "I20240215 10:18:54.886636   118 compile_pass_manager.cpp:353] [UNILOG][INFO] Debug mode: null\n",
      "I20240215 10:18:54.886639   118 compile_pass_manager.cpp:357] [UNILOG][INFO] Target architecture: DPUCAHX8L_ISA0_SP\n",
      "I20240215 10:18:54.886684   118 compile_pass_manager.cpp:465] [UNILOG][INFO] Graph name: nndct_dense_YhZNg9PeSxKQMUHt, with op num: 8\n",
      "I20240215 10:18:54.886687   118 compile_pass_manager.cpp:478] [UNILOG][INFO] Begin to compile...\n",
      "I20240215 10:18:54.889720   118 compile_pass_manager.cpp:489] [UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "I20240215 10:18:54.889736   118 compile_pass_manager.cpp:504] [UNILOG][INFO] Compile done.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(0, 2, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#dummy_input = torch.randn(1, 128, 3, 3)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minspector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minspect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/apis.py:248\u001b[0m, in \u001b[0;36mInspector.inspect\u001b[0;34m(self, module, input_args, device, output_dir, verbose_level, image_format)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minspect\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, \n\u001b[1;32m    242\u001b[0m             input_args: Union[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[Any]], \n\u001b[1;32m    243\u001b[0m             device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m    244\u001b[0m             output_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantize_result\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    245\u001b[0m             verbose_level: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m    246\u001b[0m             image_format: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    247\u001b[0m   NndctScreenLogger()\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=>Start to inspect model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inspector_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m image_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     available_format \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/hardware_v3/inspector.py:58\u001b[0m, in \u001b[0;36mInspectorImpl.inspect\u001b[0;34m(self, module, input_args, device, output_dir, verbose_level)\u001b[0m\n\u001b[1;32m     56\u001b[0m   node\u001b[38;5;241m.\u001b[39mtarget_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device_allocator\u001b[38;5;241m.\u001b[39mget_node_device(node\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#   print(node.name, node.op.type, node.target_device.get_device_type())\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach_extra_node_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_partition_result_on_screen(dev_graph, output_dir, verbose_level)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_txt(dev_graph, output_dir)\n",
      "File \u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/hardware_v3/inspector.py:236\u001b[0m, in \u001b[0;36mInspectorImpl._attach_extra_node_msg\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m NNDCT_OP\u001b[38;5;241m.\u001b[39mPERMUTE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m([kw \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswim_transpose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msink_transpose\u001b[39m\u001b[38;5;124m\"\u001b[39m]]):\n\u001b[1;32m    235\u001b[0m   order \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mnode_attr(node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mAttrName\u001b[38;5;241m.\u001b[39mORDER)\n\u001b[0;32m--> 236\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_msgs[node]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantizer insert this permute operation to convert data layout \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranspose_order_to_msg[\u001b[38;5;28mtuple\u001b[39m(order)]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for deployment.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 2, 1)"
     ]
    }
   ],
   "source": [
    "# Specify a target name or fingerprint you want to deploy on\n",
    "target = \"DPUCAHX8L_ISA0_SP\"\n",
    "#DPUCVDX8G\n",
    "# Initialize inspector with target\n",
    "inspector = Inspector(target)\n",
    "# Note: visualization of inspection results relies on the dot engine.If you don't install dot successfully, set 'image_format = None' when inspecting.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN1D(2)\n",
    "\n",
    "#dummy_data, labels = train_loader\n",
    "data_dummy = []\n",
    "for data, labels in train_loader:\n",
    "    data_dummy = data\n",
    "    break\n",
    "\n",
    "#dummy_input = torch.randn(1, 128, 3, 3)\n",
    "inspector.inspect(model, (data_dummy,), device=device, output_dir=\"inspect\", image_format=\"png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d97a4-c15a-4074-9d1e-5edf7b462d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/822561179.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(sample, dtype=torch.float32)\n",
      "/tmp/ipykernel_118/822561179.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  min_vals = torch.tensor(sample.min(axis=1).values, dtype=torch.float32)\n",
      "/tmp/ipykernel_118/822561179.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_vals = torch.tensor(sample.max(axis=1).values, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.0055\n",
      "Epoch [2/10] Loss: 0.0350\n",
      "Epoch [3/10] Loss: 0.0034\n",
      "Epoch [4/10] Loss: 0.2987\n",
      "Epoch [5/10] Loss: 1.3541\n",
      "Epoch [6/10] Loss: 0.0019\n",
      "Epoch [7/10] Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# loss function and optimizer definition\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model in training mode\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(data)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    # print loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}')\n",
    "\n",
    "# evaluate the model on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # predicted and actual labels\n",
    "        for i in range(len(labels)):\n",
    "            print(f'Predicted: {predicted[i]}, Actual: {labels[i]}')\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8825c3-893b-43d8-979c-19db391196bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5db792-d1f4-4976-b863-5fcec220b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"trainedModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
